{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPZ+kfPYm+3hXJEj9+XGRX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdiNel0407/us-ie-big-data-technologies/blob/main/postblock2/q5/postblock2_q5_1_multi_forecast_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvLNiBuHojvo",
        "outputId": "5f42bd84-f1b8-4cf2-e179-67ea868048bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/10.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/10.7 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m7.8/10.7 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSpark: 3.5.1\n"
          ]
        }
      ],
      "source": [
        "# === Q5.1 — Many models in parallel with Spark (Colab/local prototype) ===\n",
        "# - Ingest data (or synthesize if not provided)\n",
        "# - Partition by (store_id, item_id)\n",
        "# - Fit & forecast per group in parallel via grouped Pandas UDF\n",
        "# - Persist forecasts to local filesystem\n",
        "# - Evaluate each model (evaluate_forecast) and print results\n",
        "\n",
        "# 0) Environment\n",
        "!pip -q install pyspark==3.5.1 statsmodels==0.14.2\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = (SparkSession.builder\n",
        "         .appName(\"multi-forecast-parallel\")\n",
        "         .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
        "         .getOrCreate())\n",
        "print(\"Spark:\", spark.version)\n",
        "\n",
        "from pyspark.sql import functions as F, types as T\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, pathlib, datetime as dt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Ingest data from source (CSV) or synthesize if not found\n",
        "# Expected schema if reading: store_id (int/str), item_id (int/str), ds (date/str), y (float / demand)\n",
        "\n",
        "DATA_PATH = \"/content/data/store_item_demand.csv\"   # <-- change if you have a real file\n",
        "pathlib.Path(\"/content/data\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def load_or_make_data():\n",
        "    if os.path.exists(DATA_PATH):\n",
        "        dfp = pd.read_csv(DATA_PATH)\n",
        "        # normalize column names\n",
        "        cols = {c.lower():c for c in dfp.columns}\n",
        "        # ensure expected names exist\n",
        "        rename_map = {}\n",
        "        for want in [\"store_id\",\"item_id\",\"ds\",\"y\"]:\n",
        "            # try exact first, then any lower-cased match\n",
        "            if want in dfp.columns:\n",
        "                continue\n",
        "            elif want in [c.lower() for c in dfp.columns]:\n",
        "                # find original\n",
        "                orig = [c for c in dfp.columns if c.lower()==want][0]\n",
        "                rename_map[orig] = want\n",
        "        if rename_map:\n",
        "            dfp = dfp.rename(columns=rename_map)\n",
        "        # type cleanup\n",
        "        dfp[\"ds\"] = pd.to_datetime(dfp[\"ds\"])\n",
        "        dfp[\"y\"]  = pd.to_numeric(dfp[\"y\"], errors=\"coerce\").astype(float)\n",
        "        return dfp\n",
        "\n",
        "    # --- synthesize weekly data for 50 stores x 20 items x 104 weeks ---\n",
        "    rng = np.random.default_rng(7)\n",
        "    stores, items = range(1, 51), range(1, 21)\n",
        "    weeks = pd.date_range(\"2019-01-06\", periods=104, freq=\"W-SUN\")\n",
        "    rows = []\n",
        "    for s in stores:\n",
        "        for it in items:\n",
        "            base = 20 + 5*np.sin(np.arange(len(weeks))/6) + 0.1*s + 0.05*it\n",
        "            noise = rng.normal(0, 2.5, size=len(weeks))\n",
        "            y = np.maximum(0, base + noise)\n",
        "            rows.append(pd.DataFrame({\n",
        "                \"store_id\": s,\n",
        "                \"item_id\":  it,\n",
        "                \"ds\": weeks,\n",
        "                \"y\": y\n",
        "            }))\n",
        "    dfp = pd.concat(rows, ignore_index=True)\n",
        "    # Also save so you can inspect/restart quickly\n",
        "    dfp.to_csv(DATA_PATH, index=False)\n",
        "    return dfp\n",
        "\n",
        "pdf_raw = load_or_make_data()\n",
        "print(pdf_raw.head())\n",
        "print(pdf_raw.shape, pdf_raw[\"ds\"].min(), pdf_raw[\"ds\"].max())\n",
        "\n",
        "sdf_raw = spark.createDataFrame(pdf_raw)\\\n",
        "               .withColumn(\"ds\", F.to_timestamp(\"ds\"))\n",
        "sdf_raw.cache().count()\n",
        "sdf_raw.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcz5fK__owb3",
        "outputId": "0c1fb022-96ef-4f7d-829c-6572656cc3f3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   store_id  item_id         ds          y\n",
            "0         1        1 2019-01-06  20.153075\n",
            "1         1        1 2019-01-13  21.726345\n",
            "2         1        1 2019-01-20  21.100629\n",
            "3         1        1 2019-01-27  20.320648\n",
            "4         1        1 2019-02-03  22.105172\n",
            "(104000, 4) 2019-01-06 00:00:00 2020-12-27 00:00:00\n",
            "root\n",
            " |-- store_id: long (nullable = true)\n",
            " |-- item_id: long (nullable = true)\n",
            " |-- ds: timestamp (nullable = true)\n",
            " |-- y: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Prepare data & choose horizon\n",
        "# We'll forecast the last H periods per group and train on the rest.\n",
        "H = 8   # forecast horizon (e.g., 8 weeks)\n",
        "\n",
        "# For convenience, compute the per-group cutoff date (max(ds) - H + 1 step)\n",
        "# We'll make this inside the UDF; but it's handy to know globally as well.\n"
      ],
      "metadata": {
        "id": "pq9Uuvk4o7pb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "from pyspark.sql import functions as F, types as T\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "lSBqjF2SpWOE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_schema = T.StructType([\n",
        "    T.StructField(\"store_id\", T.IntegerType(), False),\n",
        "    T.StructField(\"item_id\",  T.IntegerType(), False),\n",
        "    T.StructField(\"ds\",       T.TimestampType(), False),\n",
        "    T.StructField(\"yhat\",     T.DoubleType(), False),\n",
        "])\n",
        "\n",
        "@pandas_udf(forecast_schema, PandasUDFType.GROUPED_MAP)   # <-- enum, not string\n",
        "def fit_and_forecast(pdf: pd.DataFrame) -> pd.DataFrame:\n",
        "    pdf = pdf.sort_values(\"ds\")\n",
        "    sid = int(pdf[\"store_id\"].iloc[0])\n",
        "    iid = int(pdf[\"item_id\"].iloc[0])\n",
        "\n",
        "    last_ds = pd.to_datetime(pdf[\"ds\"].max())\n",
        "    freq = pd.infer_freq(pdf[\"ds\"]) or \"W-SUN\"\n",
        "    from pandas.tseries.frequencies import to_offset\n",
        "    future_ds = pd.date_range(last_ds + to_offset(freq), periods=H, freq=freq)\n",
        "\n",
        "    y = pdf[\"y\"].astype(float).values\n",
        "    if len(y) < max(13, H):\n",
        "        yhat = np.repeat(float(np.nanmean(y)), H)\n",
        "    else:\n",
        "        from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "        try:\n",
        "            model = ExponentialSmoothing(y, trend=\"add\", seasonal=\"add\", seasonal_periods=52).fit(optimized=True)\n",
        "            yhat = model.forecast(H)\n",
        "        except Exception:\n",
        "            yhat = np.repeat(float(np.nanmean(y)), H)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"store_id\": sid,\n",
        "        \"item_id\":  iid,\n",
        "        \"ds\":       future_ds.tz_localize(None),\n",
        "        \"yhat\":     np.asarray(yhat, dtype=float),\n",
        "    })\n"
      ],
      "metadata": {
        "id": "ZSVcNeP_o_IX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "holdout_schema = T.StructType([\n",
        "    T.StructField(\"store_id\", T.IntegerType(), False),\n",
        "    T.StructField(\"item_id\",  T.IntegerType(), False),\n",
        "    T.StructField(\"ds\",       T.TimestampType(), False),\n",
        "    T.StructField(\"yhat\",     T.DoubleType(), False),\n",
        "])\n",
        "\n",
        "@pandas_udf(holdout_schema, PandasUDFType.GROUPED_MAP)    # <-- enum here too\n",
        "def backtest_fit_forecast(pdf: pd.DataFrame) -> pd.DataFrame:\n",
        "    pdf = pdf.sort_values(\"ds\")\n",
        "    sid = int(pdf[\"store_id\"].iloc[0]); iid = int(pdf[\"item_id\"].iloc[0])\n",
        "    if len(pdf) <= H:\n",
        "        return pd.DataFrame(columns=[\"store_id\",\"item_id\",\"ds\",\"yhat\"])\n",
        "\n",
        "    train = pdf.iloc[:-H]\n",
        "    valid = pdf.iloc[-H:]\n",
        "    y_train = train[\"y\"].astype(float).values\n",
        "    future_ds = pd.to_datetime(valid[\"ds\"].values)\n",
        "\n",
        "    if len(y_train) < max(13, H):\n",
        "        yhat = np.repeat(float(np.nanmean(y_train)), H)\n",
        "    else:\n",
        "        from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "        try:\n",
        "            model = ExponentialSmoothing(y_train, trend=\"add\", seasonal=\"add\", seasonal_periods=52).fit(optimized=True)\n",
        "            yhat = model.forecast(H)\n",
        "        except Exception:\n",
        "            yhat = np.repeat(float(np.nanmean(y_train)), H)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"store_id\": sid,\n",
        "        \"item_id\":  iid,\n",
        "        \"ds\":       future_ds.tz_localize(None),\n",
        "        \"yhat\":     np.asarray(yhat, dtype=float),\n",
        "    })\n"
      ],
      "metadata": {
        "id": "QzaQsLwrphCz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forecasts = (\n",
        "    sdf_raw\n",
        "    .groupBy(\"store_id\", \"item_id\")\n",
        "    .apply(fit_and_forecast)\n",
        ")\n",
        "forecasts.cache().count()\n",
        "forecasts.orderBy(\"store_id\",\"item_id\",\"ds\").show(6, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok5sPSSUpjfe",
        "outputId": "205ebb1e-8ed0-4633-bb52-1c66e6f1445a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/sql/pandas/group_ops.py:104: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+-------------------+------------------+\n",
            "|store_id|item_id|ds                 |yhat              |\n",
            "+--------+-------+-------------------+------------------+\n",
            "|1       |1      |2021-01-03 00:00:00|18.432629908086195|\n",
            "|1       |1      |2021-01-10 00:00:00|19.52189844319622 |\n",
            "|1       |1      |2021-01-17 00:00:00|17.888720210849673|\n",
            "|1       |1      |2021-01-24 00:00:00|18.196130749170198|\n",
            "|1       |1      |2021-01-31 00:00:00|17.741818604821212|\n",
            "|1       |1      |2021-02-07 00:00:00|17.875982660232278|\n",
            "+--------+-------+-------------------+------------------+\n",
            "only showing top 6 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bt_forecasts = (\n",
        "    sdf_raw\n",
        "    .groupBy(\"store_id\",\"item_id\")\n",
        "    .apply(backtest_fit_forecast)\n",
        "    .cache()\n",
        ")\n",
        "bt_forecasts.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_W6HuB0qYP0",
        "outputId": "b9b3abcd-07e9-4daa-bfe1-7d3edc5f7e7d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FORECAST_DIR = \"/content/forecasts_parquet\"\n",
        "(forecasts\n",
        "  .repartition(1)               # just to get a single file for convenience\n",
        "  .write.mode(\"overwrite\")\n",
        "  .parquet(FORECAST_DIR))\n",
        "print(\"Saved forecasts to:\", FORECAST_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAOQns-mqyza",
        "outputId": "0afa9f7f-de74-4647-b38f-67c40642a58c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved forecasts to: /content/forecasts_parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F, Window\n",
        "\n",
        "H = 8  # same horizon you used\n",
        "\n",
        "w = Window.partitionBy(\"store_id\",\"item_id\").orderBy(F.col(\"ds\").desc())\n",
        "lastH_actuals = (sdf_raw\n",
        "    .withColumn(\"rn\", F.row_number().over(w))\n",
        "    .where(F.col(\"rn\") <= H)\n",
        "    .drop(\"rn\"))\n",
        "\n",
        "eval_df = (bt_forecasts.alias(\"f\")\n",
        "           .join(lastH_actuals.alias(\"a\"),\n",
        "                 on=[\"store_id\",\"item_id\",\"ds\"], how=\"inner\")\n",
        "           .select(\"store_id\",\"item_id\",\"ds\",\"a.y\",\"f.yhat\")\n",
        "           .cache())\n",
        "eval_df.show(6, truncate=False)\n",
        "print(\"eval_df rows:\", eval_df.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz5c5Ry7q1Du",
        "outputId": "ef9592b7-1ea4-4f45-817c-1dafc26b7908"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+-------------------+------------------+-----------------+\n",
            "|store_id|item_id|ds                 |y                 |yhat             |\n",
            "+--------+-------+-------------------+------------------+-----------------+\n",
            "|4       |19     |2020-12-27 00:00:00|17.424108517592984|22.19792189600221|\n",
            "|4       |19     |2020-12-20 00:00:00|10.24830665236501 |22.19792189600221|\n",
            "|4       |19     |2020-12-13 00:00:00|16.880208187806403|22.19792189600221|\n",
            "|4       |19     |2020-12-06 00:00:00|15.069959663021121|22.19792189600221|\n",
            "|4       |19     |2020-11-29 00:00:00|16.548755516507995|22.19792189600221|\n",
            "|4       |19     |2020-11-22 00:00:00|20.28239288922161 |22.19792189600221|\n",
            "+--------+-------+-------------------+------------------+-----------------+\n",
            "only showing top 6 rows\n",
            "\n",
            "eval_df rows: 8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F, types as T\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "metric_schema = T.StructType([\n",
        "    T.StructField(\"store_id\", T.IntegerType(), False),\n",
        "    T.StructField(\"item_id\",  T.IntegerType(), False),\n",
        "    T.StructField(\"rmse\",     T.DoubleType(), True),\n",
        "    T.StructField(\"mape\",     T.DoubleType(), True),\n",
        "    T.StructField(\"smape\",    T.DoubleType(), True),\n",
        "    T.StructField(\"n\",        T.IntegerType(), True),\n",
        "])\n",
        "\n",
        "def evaluate_forecast(pdf: pd.DataFrame) -> pd.DataFrame:\n",
        "    sid = int(pdf[\"store_id\"].iloc[0])\n",
        "    iid = int(pdf[\"item_id\"].iloc[0])\n",
        "    y    = pdf[\"y\"].astype(float).values\n",
        "    yhat = pdf[\"yhat\"].astype(float).values\n",
        "    n = len(y)\n",
        "\n",
        "    rmse = float(np.sqrt(np.mean((y - yhat)**2))) if n else np.nan\n",
        "    mask = (y != 0)\n",
        "    mape = float(np.mean(np.abs((y[mask] - yhat[mask]) / y[mask]))) if mask.any() else np.nan\n",
        "    denom = (np.abs(y) + np.abs(yhat))\n",
        "    mask2 = (denom != 0)\n",
        "    smape = float(np.mean(2*np.abs(y - yhat)[mask2] / denom[mask2])) if mask2.any() else np.nan\n",
        "\n",
        "    return pd.DataFrame([{\n",
        "        \"store_id\": sid, \"item_id\": iid, \"rmse\": rmse, \"mape\": mape, \"smape\": smape, \"n\": n\n",
        "    }])\n",
        "\n",
        "per_model_metrics = (\n",
        "    eval_df\n",
        "    .groupBy(\"store_id\",\"item_id\")\n",
        "    .applyInPandas(evaluate_forecast, schema=metric_schema)\n",
        "    .orderBy(\"rmse\")\n",
        ")\n",
        "\n",
        "per_model_metrics.show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J2Nje9vq59b",
        "outputId": "8d78bbf4-cda9-4475-967d-3ada00c8a619"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+------------------+-------------------+-------------------+---+\n",
            "|store_id|item_id|rmse              |mape               |smape              |n  |\n",
            "+--------+-------+------------------+-------------------+-------------------+---+\n",
            "|46      |6      |2.29708174780037  |0.08596043662653152|0.08107873968512481|8  |\n",
            "|19      |19     |2.3405969888022136|0.09645478653464229|0.0897401545784619 |8  |\n",
            "|48      |8      |2.3671454543765384|0.09023741841787944|0.08516959093641564|8  |\n",
            "|1       |6      |2.4725953727087333|0.12104604515369524|0.11158686789852995|8  |\n",
            "|41      |10     |2.5916782803802088|0.10461183112974799|0.09772844744624379|8  |\n",
            "|6       |15     |2.611368887409247 |0.11371421288015085|0.10427135186484802|8  |\n",
            "|35      |12     |2.6380204695697285|0.0928721790219631 |0.08527861137155032|8  |\n",
            "|50      |2      |2.6786313024117008|0.09791202239740054|0.0910024032914927 |8  |\n",
            "|42      |20     |2.7055566375180726|0.10137772776905044|0.09459250296174146|8  |\n",
            "|4       |18     |2.8005839029939446|0.1284439316488981 |0.11947208393705797|8  |\n",
            "+--------+-------+------------------+-------------------+-------------------+---+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall = (per_model_metrics\n",
        "           .agg(F.avg(\"rmse\").alias(\"rmse_avg\"),\n",
        "                F.avg(\"mape\").alias(\"mape_avg\"),\n",
        "                F.avg(\"smape\").alias(\"smape_avg\"),\n",
        "                F.sum(\"n\").alias(\"total_points\")))\n",
        "overall.show(truncate=False)\n",
        "\n",
        "METRICS_DIR = \"/content/forecast_metrics_parquet\"\n",
        "per_model_metrics.repartition(1).write.mode(\"overwrite\").parquet(METRICS_DIR)\n",
        "print(\"Saved metrics to:\", METRICS_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oxbULCfrTyz",
        "outputId": "160a166a-3caa-457e-9a6d-2960b664b610"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-------------------+------------+\n",
            "|rmse_avg         |mape_avg           |smape_avg          |total_points|\n",
            "+-----------------+-------------------+-------------------+------------+\n",
            "|4.941213258390197|0.24849697962337464|0.21027958835810126|8000        |\n",
            "+-----------------+-------------------+-------------------+------------+\n",
            "\n",
            "Saved metrics to: /content/forecast_metrics_parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "overall = (per_model_metrics\n",
        "           .agg(F.avg(\"rmse\").alias(\"rmse_avg\"),\n",
        "                F.avg(\"mape\").alias(\"mape_avg\"),\n",
        "                F.avg(\"smape\").alias(\"smape_avg\"),\n",
        "                F.sum(\"n\").alias(\"total_points\")))\n",
        "overall.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy1XZFV1riwh",
        "outputId": "111cb738-0e05-4e0c-94ce-8a9d2bf9c37d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-------------------+------------+\n",
            "|rmse_avg         |mape_avg           |smape_avg          |total_points|\n",
            "+-----------------+-------------------+-------------------+------------+\n",
            "|4.941213258390197|0.24849697962337464|0.21027958835810126|8000        |\n",
            "+-----------------+-------------------+-------------------+------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}